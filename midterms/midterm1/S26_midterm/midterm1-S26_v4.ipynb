{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# DATA 271 Midterm 1 â€” Spring 2026\n",
    "*Instructor: Emily Chang*  \n",
    "*Date: Tuesday, February 24, 2026*\n",
    "\n",
    "Please answer the following questions to the best of your ability. You are welcome to use your reference sheet and the `help()` function in Python, but please do **not** use any other internet windows including books, demos from class, AI, Stack Overflow, or others.\n",
    "\n",
    "You have **1 hour and 50 minutes** to complete this exam. There is a total of **120 points**. When you have finished, click **File â†’ Download as â†’ Notebook (.ipynb)** and submit to Canvas.\n",
    "\n",
    "**Some helpful reminders:**\n",
    "- Look up documentation with the `help` function: `help(len)`\n",
    "- Look up methods and attributes with `dir`: `dir('a string')`\n",
    "- Use the `tab` key to find functions and methods for specific libraries\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1header",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: Python Basics â€” Flight Data (25 points)\n",
    "\n",
    "You are given the list `flights` below. Each element is a tuple containing `(airline, origin_city, destination_city, departure_delay_minutes)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = [\n",
    "    ('Delta Air Lines', 'New York', 'Los Angeles', 12),\n",
    "    ('United Airlines', 'Chicago', 'Houston', -5),\n",
    "    ('Southwest Airlines', 'Dallas', 'Phoenix', 34),\n",
    "    ('Delta Air Lines', 'Los Angeles', 'Seattle', 0),\n",
    "    ('American Airlines', 'New York', 'Miami', 78),\n",
    "    ('United Airlines', 'Houston', 'Denver', -3),\n",
    "    ('Southwest Airlines', 'Phoenix', 'Las Vegas', 21),\n",
    "    ('American Airlines', 'Miami', 'Chicago', 5),\n",
    "    ('Delta Air Lines', 'Seattle', 'New York', 15),\n",
    "    ('United Airlines', 'Denver', 'Chicago', 0),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1q1text",
   "metadata": {},
   "source": [
    "**1.1** Using list comprehension, create a list called `delayed_flights` containing only the tuples where the departure delay is greater than 0. **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1q1",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_flights = ...\n",
    "delayed_flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1q2text",
   "metadata": {},
   "source": [
    "**1.2** Create a dictionary called `airline_delays` where the keys are unique airline names and the values are **lists** of all departure delays (in minutes) for that airline. **(8 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1q2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_airlines = \n",
    "\n",
    "airline_delays = {}\n",
    "for ... in ...:\n",
    "    ...\n",
    "airline_delays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1q3text",
   "metadata": {},
   "source": [
    "**1.3** Write a function called `avg_delay` that takes the `airline_delays` dictionary and an airline name as arguments, and returns the **average delay** (as a float) for that airline. Call your function for `'Delta Air Lines'` and assign the result to `delta_avg`. **(6 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1q3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_delay(delay_dict, airline):\n",
    "    ...\n",
    "\n",
    "delta_avg = ...\n",
    "delta_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1q4text",
   "metadata": {},
   "source": [
    "**1.4** Using `sorted()` and a lambda function, sort the `flights` list by departure delay in **descending** order (longest delay first). Assign the result to `flights_sorted`. **(6 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p1q4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_sorted = ...\n",
    "flights_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2header",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Regular Expressions â€” Date Cleaning (25 points)\n",
    "\n",
    "You are given `data`, a list of dates in several different formats. Your goal is to standardize all of them to `YYYY-MM-DD` and sort them chronologically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data = ['1-1-2023',\n",
    " 'Jan 4th 2023',\n",
    " 'Jan 6 2023',\n",
    " '01-10-2023',\n",
    " '01/12/23',\n",
    " 'January 14th 2023',\n",
    " 'Jan 16 2023',\n",
    " 'Jan 17 2023',\n",
    " '01/20/2023',\n",
    " '01/22/23',\n",
    " 'January 24 2023',\n",
    " 'Jan 25 2023',\n",
    " '1-28-23',\n",
    " '01-29-2023',\n",
    " 'February 1st 2023',\n",
    " 'Feb 3 2023',\n",
    " 'Feb 5 2023',\n",
    " '02-06-2023',\n",
    " '02-6-2023',\n",
    " '2-10-23']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2q1text",
   "metadata": {},
   "source": [
    "**2.1** Look at the `data` list above. There are **4 distinct date formats** mixed together. In the Markdown cell below, describe each format group and give one example from the list for each. **(4 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2q1ans",
   "metadata": {},
   "source": [
    "*Format 1:*\n",
    "\n",
    "*Format 2:*\n",
    "\n",
    "*Format 3:*\n",
    "\n",
    "*Format 4:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2q2text",
   "metadata": {},
   "source": [
    "**2.2** Some dates have **2-digit years** (e.g. `'23'`) and others have **4-digit years** (e.g. `'2023'`). Write a helper function called `fix_year` that takes a year string and returns a 4-digit year string. Assume all 2-digit years belong to the 2000s. **(5 points)**\n",
    "\n",
    "*Example:* `fix_year('23')` â†’ `'2023'`,  `fix_year('2023')` â†’ `'2023'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2q2code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_year(year_str):\n",
    "    ...\n",
    "\n",
    "# Test it:\n",
    "print(fix_year('23'))    # Expected: '2023'\n",
    "print(fix_year('2023'))  # Expected: '2023'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2q3text",
   "metadata": {},
   "source": [
    "**2.3** Handle the **numeric formats** (both dash-separated and slash-separated). Use `re.findall()` with capturing groups to extract the month, day, and year from each date. Use your `fix_year` function and zero-padding to produce a standardized `YYYY-MM-DD` string.\n",
    "\n",
    "Create a list called `numeric_dates` containing only the dates from `data` that match a numeric format, converted to `YYYY-MM-DD`. You may use a loop or list comprehension. **(5 points)**\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "['2023-01-01', '2023-01-10', '2023-01-12', '2023-01-20', '2023-01-22',\n",
    " '2023-01-28', '2023-01-29', '2023-02-06', '2023-02-06', '2023-02-10']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2q3code",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_dates = ...\n",
    "numeric_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2q4text",
   "metadata": {},
   "source": [
    "**2.4** Handle the **written month formats** (both abbreviated like `'Jan'` and full like `'January'`). Use `re.findall()` to extract the month name, day number, and year from each date.\n",
    "\n",
    "- Create a dictionary called `month_map` that maps month names to their zero-padded numeric equivalents (only cover the months that appear in `data`)\n",
    "- Strip ordinal suffixes (`st`, `nd`, `rd`, `th`) from day numbers\n",
    "- Use `fix_year` for the year\n",
    "\n",
    "Create a list called `written_dates` containing only the dates from `data` that match a written month format, converted to `YYYY-MM-DD`. You may use a loop or list comprehension. **(6 points)**\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "['2023-01-04', '2023-01-06', '2023-01-14', '2023-01-16', '2023-01-17',\n",
    " '2023-01-24', '2023-01-25', '2023-02-01', '2023-02-03', '2023-02-05']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2q4code",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_map = {...}\n",
    "\n",
    "written_dates = ...\n",
    "written_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2q5text",
   "metadata": {},
   "source": [
    "**2.5** Combine `numeric_dates` and `written_dates` into one list called `all_dates`. Then sort `all_dates` from **earliest to latest** and assign the result to `sorted_dates`. **(5 points)**\n",
    "\n",
    "```python\n",
    "# Expected (20 dates, Jan 1 through Feb 10):\n",
    "# ['2023-01-01', '2023-01-04', '2023-01-06', '2023-01-10', '2023-01-12',\n",
    "#  '2023-01-14', '2023-01-16', '2023-01-17', '2023-01-20', '2023-01-22',\n",
    "#  '2023-01-24', '2023-01-25', '2023-01-28', '2023-01-29', '2023-02-01',\n",
    "#  '2023-02-03', '2023-02-05', '2023-02-06', '2023-02-06', '2023-02-10']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2q5code",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dates = ...\n",
    "\n",
    "sorted_dates = sorted(all_dates, key=...)\n",
    "sorted_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p3header",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: NumPy â€” Minor Theatre Snacks (25 points)\n",
    "\n",
    "You're at the Minor Theatre in Arcata getting snacks and watching *Wuthering Heights*. Unfortunately, the menu is given to you as two 2D arrays below:\n",
    "\n",
    "- `menu_items` â€” a 2D array of concession items organized by category:\n",
    "  - Row 0: snacks\n",
    "  - Row 1: meals\n",
    "  - Row 2: drinks\n",
    "\n",
    "- `prices` â€” a 2D array of the **same shape** as `menu_items`, containing the price (in dollars) of each corresponding item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p3setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "menu_items = np.array([\n",
    "    ['gingersnap 3-pack', 'yeast popcorn',  'Dick Taylor bar',  'Bavarian pretzel'],\n",
    "    ['Smugs pizza',       'signature dog',  'empanada',         'nachos'          ],\n",
    "    ['kombucha',          'hot chocolate',  'herbal tea',       'beer float'      ]\n",
    "])\n",
    "\n",
    "prices = np.array([\n",
    "    [ 6.,  8., 10.,  7.],\n",
    "    [ 6.,  7.,  7.,  6.],\n",
    "    [ 7.,  6.,  5.,  9.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p3q1text",
   "metadata": {},
   "source": [
    "**3.1** Use NumPy methods to determine how much someone would pay if they ordered the **most expensive item from each category** (one snack, one meal, one drink). Assign the total to `max_order_total`. **(6 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p3q1code",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_order_total = ...\n",
    "max_order_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p3q2text",
   "metadata": {},
   "source": [
    "**3.2** Use NumPy methods and slicing to create a 1D array called `items_by_price` containing **all menu items sorted from most expensive to least expensive**. **(7 points)**\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "['Dick Taylor bar' 'beer float' 'yeast popcorn' 'signature dog' 'kombucha'\n",
    " 'empanada' 'Bavarian pretzel' 'nachos' 'hot chocolate' 'gingersnap 3-pack'\n",
    " 'Smugs pizza' 'herbal tea']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p3q2code",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_by_price = ...\n",
    "items_by_price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p3q3text",
   "metadata": {},
   "source": [
    "**3.3** Use NumPy methods and slicing to create a 1D array called `priciest_items` containing the **name** of the most expensive item from each category (one snack, one meal, one drink). **(7 points)**\n",
    "\n",
    "Expected output:\n",
    "```\n",
    "['Dick Taylor bar' 'signature dog' 'beer float']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p3q3code",
   "metadata": {},
   "outputs": [],
   "source": [
    "priciest_items = ...\n",
    "priciest_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p3q4text",
   "metadata": {},
   "source": [
    "**3.4** Use Boolean masking to find all items that cost **more than $6.00**. Assign the item names to `pricey_items` and the count to `num_pricey`. **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p3q4code",
   "metadata": {},
   "outputs": [],
   "source": [
    "pricey_items = ...\n",
    "num_pricey   = ...\n",
    "\n",
    "pricey_items, num_pricey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4header",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Pandas Basics â€” Flight Delays Dataset (25 points)\n",
    "\n",
    "For this problem, use the `flight_delays.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4q1text",
   "metadata": {},
   "source": [
    "**4.1** Load `flight_delays.csv` into a DataFrame called `flights_df`. Display the first 5 rows to confirm it loaded correctly. Then report the shape of the dataset and assign it to `flights_shape`. **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4q1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = ...\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4q1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_shape = ...\n",
    "flights_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4q2text",
   "metadata": {},
   "source": [
    "**4.2** Use `.info()` and `.describe()` to inspect the dataset. In the Markdown cell below, answer: (a) Are there any columns with missing values? How do you know from `.info()`? (b) What is the mean departure delay? **(6 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4q2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4q2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4q2md",
   "metadata": {},
   "source": [
    "*(a) Missing values:*\n",
    "\n",
    "*(b) Mean departure delay:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4q3text",
   "metadata": {},
   "source": [
    "**4.3** a) Find the **average departure delay** for each airline. Use `.groupby()` and assign the result to `avg_delay_by_airline`.\n",
    "\n",
    "b) Which airline has the highest average departure delay? Assign its name to `worst_airline`.\n",
    "\n",
    "**(8 points)**\n",
    "\n",
    "*Hint: use `.idxmax()` to grab the index name of the smallest value!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4q3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "avg_delay_by_airline = ...\n",
    "avg_delay_by_airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4q3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "worst_airline = ...\n",
    "worst_airline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p4q4text",
   "metadata": {},
   "source": [
    "**4.4** How many flights were cancelled? The `Cancelled_Flag` column indicates whether a flight was cancelled. Assign the count to `num_cancelled`. Then use `.value_counts()` on the `Airline_Name` column to find how many flights each airline operated. Assign the result to `flights_per_airline`. **(6 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4q4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cancelled = ...\n",
    "num_cancelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p4q4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_per_airline = ...\n",
    "flights_per_airline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6header",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5: ETL Pipeline â€” Glassdoor Jobs Dataset (20 points)\n",
    "\n",
    "The field of data science is growing rapidly, with companies across various industries seeking skilled professionals. In this problem, you'll build a complete **data inspection and visualization pipeline** using `glassdoor_jobs.csv`, which contains data science job listings scraped from Glassdoor.\n",
    "\n",
    "You'll follow the ETL inspection flow from lecture:\n",
    "> 1. Confirm it loaded correctly â†’ 2. Inspect shape â†’ 3. Identify data types â†’ 4. Identify missing values â†’ 5. Filter meaningful subsets â†’ 6. Summarize patterns â†’ 7. Wrap it in a function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q1text",
   "metadata": {},
   "source": [
    "**5.1** Import `numpy` as `np`, `pandas` as `pd`, and `matplotlib.pyplot` as `plt` (if you haven't already). Load `glassdoor_jobs.csv` into a DataFrame called `jobs`. Display the first 5 rows to confirm it loaded correctly. Then assign the column names to `col_names`. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jobs = ...\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ...\n",
    "col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q2text",
   "metadata": {},
   "source": [
    "**5.2** Use `.info()` to get a full summary of the DataFrame, and `.describe()` to get descriptive statistics of the numeric columns. In the Markdown cell below, answer: which columns have missing values, and how can you tell from `.info()`? **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .info()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .describe()\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q2md",
   "metadata": {},
   "source": [
    "*Which columns have missing values? How do you know from `.info()` output?*\n",
    "\n",
    "(type here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q3text",
   "metadata": {},
   "source": [
    "**5.3** Assign the number of rows to `num_rows` and the number of columns to `num_cols`. Print them in the format: `\"The dataset has {num_rows} rows and {num_cols} columns.\"` **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = ...\n",
    "num_cols = ...\n",
    "print(f\"The dataset has {num_rows} rows and {num_cols} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q4text",
   "metadata": {},
   "source": [
    "**5.4** Use `select_dtypes()` to identify the numeric and categorical features. Assign the column names as lists to `numeric_cols` and `categorical_cols`. In the Markdown cell, comment on whether the numeric features loaded as expected (float/int) â€” note any surprises. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ...\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ...\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q4md",
   "metadata": {},
   "source": [
    "*Did all numeric features load as float/integer? Were there any surprises?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q5text",
   "metadata": {},
   "source": [
    "**5.5** Use `.info()` to identify which columns have missing values. Assign to `cols_with_missing` a list of column names that have fewer non-null values than `num_rows`.\n",
    "\n",
    "*HINT: Look at the \"Non-Null Count\" column in the `.info()` output and compare it to `num_rows`.* **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_missing = ...\n",
    "cols_with_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q6text",
   "metadata": {},
   "source": [
    "**5.6** Filter for meaningful subsets using four different approaches. Each should produce a separate DataFrame.\n",
    "\n",
    "**5.6a** Use **conditional filtering** to create `high_pay` â€” jobs where `max_salary` is greater than 150. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_pay = ...\n",
    "high_pay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q6btext",
   "metadata": {},
   "source": [
    "**5.6b** Use **`.between()`** to create `mid_pay` â€” jobs where `max_salary` is between 80 and 120 (inclusive). **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_pay = ...\n",
    "mid_pay.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q6ctext",
   "metadata": {},
   "source": [
    "**5.6c** Create `big_companies` containing only jobs at companies with more than 1,000 employees. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_companies = ...\n",
    "big_companies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q6dtext",
   "metadata": {},
   "source": [
    "**5.6d** Use regex to identify jobs whose `job_description` mentions `'machine learning'` or `'ML'`.\n",
    "\n",
    "Save this as a new column in the dataframe called `ml_tag`. If there is match, save the value as `ML` and if there is no match, save the value as `not ML`. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['ml_tag'] = ...\n",
    "\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q7text",
   "metadata": {},
   "source": [
    "**5.7** Calculate the count of `ML` and `not ML` jobs in the `ml_tag` column. Then create a **bar plot** for the `ml_tag` column using the `.plot(kind=\"bar\")`. Add a title with `plt.title()` and display with `plt.show()`. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2bd41-6349-4f25-9053-9064f2cc6aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_counts = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_counts.plot(kind='bar')\n",
    "plt.title(...)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q8text",
   "metadata": {},
   "source": [
    "**5.8** Summarize patterns in the **numeric** features. Loop through each column in `numeric_cols` and create a **histogram** using `jobs.plot(kind=\"hist\")`. Add a title with `plt.title()`, an x-axis label with `plt.xlabel()`, and display with `plt.show()`. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_cols:\n",
    "    ...\n",
    "    plt.title(...)\n",
    "    plt.xlabel(...)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q9text",
   "metadata": {},
   "source": [
    "**5.9 (Almost done!!)** Wrap the entire pipeline into a function called `inspect_df` that takes a DataFrame as its only argument and performs all of the following in order:\n",
    "\n",
    "1. Prints `.info()` output\n",
    "2. Prints `.describe()` output\n",
    "3. Prints the shape as: `\"Shape: {rows} rows, {cols} columns\"`\n",
    "4. Prints the list of numeric column names\n",
    "5. Prints the list of categorical column names\n",
    "6. Prints the list of columns with missing values (fewer non-null than total rows)\n",
    "7. Prints `value_counts()`, and shows a bar plot for only the `ml_tag` column\n",
    "8. Loops through numeric columns and shows a histogram for each\n",
    "\n",
    "Test your function by calling `inspect_df(jobs)`. **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_df(df):\n",
    "    # 1. info\n",
    "    ...\n",
    "    # 2. describe\n",
    "    ...\n",
    "    # 3. shape\n",
    "    ...\n",
    "    # 4. numeric cols\n",
    "    ...\n",
    "    # 5. categorical cols\n",
    "    ...\n",
    "    # 6. columns with missing values\n",
    "    ...\n",
    "    # 7. categorical bar plots\n",
    "    ...\n",
    "    # 8. numeric histograms\n",
    "    ...\n",
    "\n",
    "inspect_df(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endnote",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ‰ You're done!\n",
    "\n",
    "Please save your notebook and submit it to Canvas as a `.ipynb` file.\n",
    "\n",
    "**File â†’ Download as â†’ Notebook (.ipynb)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
